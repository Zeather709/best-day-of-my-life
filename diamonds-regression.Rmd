---
title: "Best Day of My Life"
author: "Heather Zurel"
date: "2022-09-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The best day of my life was when my partner asked me to marry him.  He was curious whether he overpaid for the diamond he bought me (he thinks he got a great deal!) - so let's see. 

I'm going to use the diamonds data set to train a regression model to predict the price of diamonds based on their physical properties, including size/carat, clarity, colour, etc. 

The first step is some **exploratory data analysis** to see if we need to do anything to clean the data before training the model.

P.S. There are some additional analyses, plots, etc. that are included in the `diamonds.r` file in github, which have not made it to this document.  

# Preparation and Exploratory Data Analysis

```{r prep, warning = FALSE, message = FALSE}
# Setup
library('tidyverse')
library('GGally')

# Import
data(diamonds)

# Exploratory 
summary(diamonds)
ggpairs(diamonds)
```

So the x, y, and z values correspond to the physical dimensions of the diamonds in millimeters - these correlate strongly with the carat value, which is a measure of the weight of the diamonds - this makes sense & we really don't need to include all of them.  Some ML models are sensitive to including multiple features which are strongly correlated with each other - this can lead to overfitting - so we will remove the x, y, and z variables before we train the model. 

### Distribution of the Target Variable (Price in USD)
Q-Q plots (or quantile-quantile plots) are used to quickly, visually identify the distribution of a single variable.  

```{r qq, echo=TRUE, warning = FALSE, message = FALSE}
qq_diamonds <- qqnorm((diamonds$price),main="Normal Q-Q Plot of Price");qqline((diamonds$price))
# Meh

qq_log_diamonds <- qqnorm(log(diamonds$price),main="Normal Q-Q Plot of log Price");qqline(log(diamonds$price))
# Ooh this is a much better fit
```
```{r hist normal, warning = FALSE, message = FALSE}
hist_norm <- ggplot(diamonds, aes(log(price)))  + 
  geom_histogram(aes(y = ..density..), colour = "black", fill = 'lightblue', bins = 50) + 
  stat_function(fun = dnorm, args = list(mean = mean(log(diamonds$price)), sd = sd(log(diamonds$price))))
hist_norm
```

Based on the Q-Q plots and the  histogram, it appears that the log of the price follows a bimodal or multimodal distribution.  Let try another couple plots to see.  

```{r violin ecdf, echo = TRUE, warning = FALSE}
violin <- ggplot(diamonds, aes(x = color, y = log(price), fill = color))
violin + geom_violin() + scale_y_log10() + facet_grid(clarity ~ cut)

carat <- ggplot(data = diamonds, aes(x = carat, y = log(price), colour = color)) 
carat + stat_ecdf() + facet_grid(clarity ~ cut)
```

Yep, this definitely looks like a multimodal distribution with multiple peaks in the distribution corresponding to increasing the carat of the diamond from 0.99 to 1 carat, 1.99 to 2 carats, etc. And smaller jumps around 1/2 carats. 

### Variance of Numeric Variables

One more thing I'm going to check is the variance of the numeric variables.  If the variance of any variable is >= 1 order of magnitude different from the others, we will standardize these values.  If the variance of one variable is much larger than others, it can over-emphasize the importance of these variables in training models. 

```{r var}
diamonds %>% summarise_if(is.numeric, list(mean = mean, var = var)) %>% t()
```

The `carat` variable is > 1 order of magnitude less than that of the `table` variable (and so close to 1 OOM smaller than `depth`), so we will go ahead and standardize table and depth. However this should occur after the data set has been split into the training and testing data sets.  

I think I have enough info for the next step...

# Data Cleaning

We are going to remove some variables that are strongly correlated with each other, leaving a single variable which captures that data contained in the other 3 variables.  

We are going to convert the price to the log of the price. Since this data set is from 2017 and I am trying to predict the value of a diamond bought in 2021, we will also adjust for inflation (approx 10.55%).

Another important consideration before training the models is to deal with the categorical data.  Often, these will be converted to "dummy variables" or one-hot-encoded.  This works when there is no natural ranking or order of the categories.  Here, the cut, clarity, and color all have a natural order.  For example, a diamond with a "good" cut is better than a diamond with a "fair" cut.  If you imported this data from r (`data(diamonds)`) then these variables will already be factors with the correct order.  However, if you downloaded a csv of this data set, these will need to be converted from strings to ordered factors, so I will include the transformation step here (even though it shouldn't change anything in my data set - though  it appears that the 'color' variable is in the reverse order so I'll fix that too). Note: the levels in this function are assigned from worst to best.  

After investigating the table and depth fields some more, these values are the ratio to the average diameter of the diamond. The table % influences the light performance of the diamond (i.e. how sparkly it looks).  The depth % affect the diamond's brilliance and fire (I'm not 100% sure what this means, but we'll see if it affects price). 

```{r clean, echo = TRUE, warning = FALSE}
diamonds <- diamonds %>%
  mutate(price = price * 1.1055) %>%
  mutate(log_price = log(price)) %>%
  select(-price, -x, -y, -z) %>%
  mutate(cut = factor(cut, levels = c('Fair', 'Good', 'Very Good', 'Premium', 'Ideal'), ordered = TRUE),
         color = factor(color, levels = c('J', 'I', 'H', 'G', 'F', 'E', 'D'), ordered = TRUE),
         clarity = factor(clarity, levels = c('I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF'), ordered = TRUE))

# This should show that these three variables are now ordered factors.
str(diamonds)

# Here's a little trick to get R to output the order all possible factor levels, instead of just the first few:
min(diamonds$cut)
min(diamonds$color)
min(diamonds$clarity)
```
#### Looks good!  

# Model Prep & Training
Time to prepare the model for training.  We will split the data into the training and testing data sets. 

Note: we also standardize (scale) the data after splitting  the data set to avoid **data leakage**.  This means that the values of the training data set are affecting the values of the testing data set because their values are used in the standardization step.  This can affect the performance of the model when running the model on new data. 

```{r ml prep}
# Prep
library(caTools)
library(tictoc)

set.seed(42)

tic.clearlog()

split <- sample.split(diamonds$log_price, SplitRatio = 0.8)
diamonds_train <- subset(diamonds, split == TRUE)
diamonds_test <- subset(diamonds, split == FALSE)

diamonds_train <- diamonds_train %>% 
  mutate_at(c('table', 'depth'), ~(scale(.) %>% as.vector))
diamonds_test <- diamonds_test %>% 
  mutate_at(c('table', 'depth'), ~(scale(.) %>% as.vector))  

glimpse(diamonds_test)

# And let's check out the standardized variables:
mean(diamonds_test$table)
sd(diamonds_test$table)

# The other one (depth) look similar too, you can use the code in diamonds.r to check for yourself. 
```

Now that the data set is split into the testing and training data sets, we will train a few different models, test their performance, and use the best one to make our prediction. 

### Multiple Linear Regression
```{r multi linear}
tic('mlm')
mlm <- lm(log_price ~ ., diamonds_train)
toc(log = TRUE, quiet = TRUE)
summary(mlm)
```

## Polynomial Regression
```{r poly}
tic('poly')
poly <- lm(log_price ~ poly(carat,3) + color + cut + clarity + poly(table,3) + poly(depth,3), diamonds_train)
toc(log = TRUE, quiet = TRUE)
summary(poly)
```
Wow!  The polynomial regression appears to fit a lot better! 

## Support Vector Regression (SVR)

SVR does not depend on distributions of the underlying dependent and independent variables.  It can also be used to construct a non-linear model with the `kernel = 'radial'` option.  I think this is the case because the linear model is performing the worst so far. 

```{r svr}
tic('svr')
library(e1071)
svr <- svm(formula = log_price ~ .,
                data = diamonds_train,
                type = 'eps-regression',
                kernel = 'radial')
toc(log = TRUE, quiet = TRUE)
```

## Decision Tree Regerssion

Decision Trees use a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model.  Training DT models can sometimes lead to over-complex trees that do not generalize the data well. This is called overfitting.

```{r tree}
tic('tree')
library(rpart)
tree <- rpart(formula = log_price ~ .,
                  data = diamonds_train,
                  method = 'anova',
                  model = TRUE)
toc(log = TRUE, quiet = TRUE)
tree
```

## Random Forest Regression

This takes the decision tree model a step further and uses many decision trees to make better predictions than if you were using any of the single decision trees in this model. 

```{r forest, warning = FALSE, message = FALSE}
tic('rf')
library(randomForest)
rf <- randomForest(log_price ~ .,
                   data = diamonds_train,
                   ntree = 500,
                   importance = TRUE)
toc(log = TRUE, quiet = TRUE)
rf
```

## XGBoost Regression

XGBoost uses gradient boosted decision trees and is a really robust model which performs very well on a broad variety of applications.  It is also not as sensitive to issues that affect the performance of some other models such as multicolinearity, or data normalization/standardization.

```{r xgb, warning = FALSE, message = FALSE, echo = TRUE, results = 'hide'}
tic('xgb')
library(xgboost)
diamonds_train_xgb <- diamonds_train %>%
  mutate_if(is.factor, as.numeric)
diamonds_test_xgb <- diamonds_test %>%
  mutate_if(is.factor, as.numeric)

xgb <- xgboost(data = as.matrix(diamonds_train_xgb[-7]), label = diamonds_train_xgb$log_price, nrounds = 6166, verbose = 0)
# the rmse stopped decreasing after 6166 rounds 
toc(log = TRUE, quiet = TRUE)
```

# Model Performance

Now we will predict the log of the price of diamonds in the test data set using each model we trained to determine which model performs best on data it has not seen.  

```{r performance}
# Make predictions and compare model performance
tic('predict_all')
mlm_pred <- predict(mlm, diamonds_test)
poly_pred <- predict(poly, diamonds_test)
svr_pred <- predict(svr, diamonds_test)
tree_pred <- predict(tree, diamonds_test)
rf_pred <- predict(rf, diamonds_test)
xgb_pred <- predict(xgb, as.matrix(diamonds_test_xgb[-7]))
toc(log = TRUE, quiet = TRUE)

# Calculate residuals (i.e. how different the predictions are from the log_price of the test data set)
xgb_resid <- diamonds_test_xgb$log_price - xgb_pred
library(modelr)
resid <- diamonds_test %>%  
  spread_residuals(mlm, poly, svr, tree, rf) %>%
  select(mlm, poly, svr, tree, rf) %>%
  rename_with( ~ paste0(.x, '_resid')) %>%
  cbind(xgb_resid)

predictions <- diamonds_test %>%
  select(log_price) %>%
  cbind(mlm_pred) %>%
  cbind(poly_pred) %>%
  cbind(svr_pred) %>%
  cbind(tree_pred) %>%
  cbind(rf_pred) %>%
  cbind(xgb_pred) %>%
  cbind(resid)           # This will be useful for plotting later

# Calculate R-squared - this describes how much of the variability is explained by the model - the closer to 1, the better

mean_log_price <- mean(diamonds_test$log_price)
tss =  sum((diamonds_test_xgb$log_price - mean_log_price)^2 )

square <- function(x) {x**2}
r2 <- function(x) {1 - x/tss}

r2_df <- resid %>%
  mutate_all(square) %>%
  summarize_all(sum) %>%
  mutate_all(r2) %>%
  gather(key = 'model', value = 'r2') %>%
  mutate(model = str_replace(model, '_resid', ''))
r2_df
```

# Visualize Performance of the Model

The Random Forest model performed best according to the R^2 value - this is a measure of how much of the variability in the data set is explained by the model, so we will mostly focus on this one for visualizations.  It is equal to 1 - RMSE (root mean square error, which describe how different all the predictions are from the true values in the `y_test` data set).

```{r vis}
library(ggplot2)
r2_plot <- ggplot(r2_df, aes(x = model, y = r2, colour = model, fill = model)) + geom_bar(stat = 'identity')
r2_plot + ggtitle('R-squared Values for each Model') + coord_cartesian(ylim = c(0.75, 1))

sample <- predictions %>%
  slice_sample(n = 1000) 
ggplot(sample, aes(x = exp(log_price), y = exp(rf_pred), size = abs(rf_resid))) +
  geom_point(alpha = 0.1) + labs(title = 'Predicted vs Actual Cost of Diamonds in USD', x = 'Price', y = 'Predicted Price', size = 'Residuals')
```

The **Random Forest** model is performing the best out of all the models we tried.  This isn't surprising, as it is an **ensemble method** which means it uses the concordance between multiple models to make better predictions than any of the models could make on their own.  XGBoost is another example of an ensemble method, and also performed very well.  In the second plot, the size is proportionate to the absolute value of the residuals, which means how different the prediction was from the real value. 

## Feature Importance 

Which variable(s) were the most important for predicting the price of the diamonds? 

```{r feature importance}
varImpPlot(rf)
```

The values on the x axis indicate how much the prediction error would increase if that variable were not included in the model.  As expected, the carat (or size) is the most important variable.  Even though table and depth are supposed to impact how sparkly the diamond appears, they don't actually have that big an effect on the price.  

## How long did it take to train models and make predictions?

```{r time}
# Training & predicting times
time_log <- tic.log(format = TRUE)
time_log
```

So the best performing models are taking the longest to train.  Not a huge surprise since they actually include many models.  Keep in mind that this is not a large data set.  Most models I have trained for work take hours and hours (I mostly set them up to run overnight).  This can be sped up with a really powerful machine in AWS or even a cluster of machines.  
